{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_2class(X, y, ylabels =['y=0','y=1'], alpha = 0.8, ax = None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap ='bwr',s = 8, alpha = alpha, edgecolors = 'none')\n",
    "    ax.legend(scatter.legend_elements()[0],ylabels , loc=\"lower right\", title=\"Classes\")\n",
    "def plot_classification_region(X_test, model,  alpha = 0.5, ylabels =['y=0','y=1'] ):\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "    XX = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    YY = model.predict(XX)\n",
    "    y_predict = model.predict(X_test)\n",
    "    fig, ax = plt.subplots()\n",
    "    scatter_2class(XX, YY, ax = ax, alpha = alpha, ylabels = ylabels)\n",
    "    scatter_2class(X_test, y_predict, ax = ax,ylabels = ylabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data \n",
    "\n",
    "We use a two-class toy dataset generated with the \"make_moons\" function from the scikit-learn library.\n",
    "## Preprocessing\n",
    "- Nomarlize data\n",
    "We normalize the data using $\\frac{X-\\mu}{\\sigma}$. \n",
    "For this, we use `StandardScaler` to ensure scaling the data. \n",
    "\n",
    "- Additionally, we randomly separate the dataset into training and testing sets using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_moons(noise=0.1, random_state=0,n_samples = 5000)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_norm = StandardScaler().fit_transform(X)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (10,4))\n",
    "scatter_2class(X,y, ax = axs[0])\n",
    "axs[0].set_title('Raw data')\n",
    "scatter_2class(X_norm, y, ax = axs[1])\n",
    "axs[1].set_title('Normalized data')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2, random_state=0)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (10,4))\n",
    "scatter_2class(X_train,y_train, ax = axs[0])\n",
    "axs[0].set_title('Training data')\n",
    "scatter_2class(X_test, y_test, ax = axs[1])\n",
    "axs[1].set_title('Test data')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector machine \n",
    "\n",
    "<!-- ## boudary \n",
    "Loss function for SVM  (Page 328)\n",
    "$$L = \\frac{1}{2}||w||^2 -\\sum_{n=1}^N a_n \\{ t_n(w^T \\phi(x_n) +b)-1\\} (7.7) $$  -->\n",
    "\n",
    "\n",
    "##  SVM Duality\n",
    "The goal of SVM is find the maximum margin solution: \n",
    "$$ \\text{argmax}_{w,b}\\{\\frac{1}{||w||} \\min [t_n(w^T\\phi(x_n) +b)]\\}  (7.3)$$\n",
    "whoes dual representation is : \n",
    "$$ \\text{argmax}_{a} \\tilde L(a)= \\sum_{n=1}^N a_n -\\frac{1}{2}\\sum_{n=1}^N \\sum_{m=1}^N a_n a_m t_n t_m k(x_n, x_m)   (7.10, 7.32)$$\n",
    "with constrants: \n",
    "$$a_n\\geq 0\\\\\n",
    "\\sum_{n=1}^N a_n t_n = 0$$\n",
    "\n",
    "The dual decision function becomes: \n",
    "$$y(x) = \\sum_{n=1}^N a_n t_n k(x, x_n) +b (7.13) $$\n",
    "\n",
    "\n",
    "## Please use Gradient Aescent to solve $a$ \n",
    "The first derivative of $L(a)$ over $a$: \n",
    "$$\\frac{\\partial L(a)}{\\partial a_k}   = 1-\\sum_{j = 1}^N a_j t_jt_k k(x_j, x_k) $$\n",
    "Using Gradient Aescent (because we need to find the maximum of $L(a)$) :\n",
    "\n",
    "$$a_k = a_k +\\eta \\frac{\\partial L(a)}{\\partial a_k} $$\n",
    "\n",
    "## Finding $b$ using support vectors\n",
    "$$b = \\frac{1}{N_s}\\sum_{n\\in S} (t_n -\\sum_{m\\in S} a_m t_m k(x_n, x_m)))$$\n",
    "where $S$ is support vectors, satisfing $t_n y(x_n) =1$. \n",
    "Using dual varaible represenation: \n",
    "$$0\\leq a_n \\leq C (7.33) $$\n",
    "where $C$ is is a trade-off parameter between misclassficaiton penalty and the soft margin (7.21).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_Dual:\n",
    "    def __init__(self, kernel='poly', degree=2, sigma=0.1, epoches=1000, learning_rate= 0.001):\n",
    "        self.b = 0\n",
    "        self.degree = degree\n",
    "        self.C = 1\n",
    "        self.sigma = sigma\n",
    "        self.epoches = epoches\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if kernel == 'poly':\n",
    "            self.kernel = self.polynomial_kernal # for polynomial kernal\n",
    "        elif kernel == 'rbf':\n",
    "            self.kernel =  self.gaussian_kernal # for guassian kernal\n",
    "\n",
    "    def polynomial_kernal(self,X,Z):\n",
    "        return (1 + X.dot(Z.T))**self.degree  \n",
    "        \n",
    "    def gaussian_kernal(self, X,Z):\n",
    "        return np.exp(-(1 / self.sigma ** 2) * np.linalg.norm(X[:, np.newaxis] - Z[np.newaxis, :], axis=2) ** 2)  \n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        C = self.C\n",
    "        a = np.random.random(X.shape[0])\n",
    "        \n",
    "        n_samples,n_features = X.shape\n",
    "        losses =[]\n",
    "        for i in range(self.epoches):\n",
    "            \n",
    "            ################### BEGIN CODE #############################\n",
    "            # you need to complete the calculation of the gradient and \n",
    "            # the update of w using gradient descent: \n",
    "         \n",
    "            # Gradient of L(a)\n",
    "            # gradient = \n",
    "            # a = \n",
    "\n",
    "             # loss function \n",
    "            #loss =  \n",
    "            #losses.append(loss)\n",
    "\n",
    "\n",
    "            # find index of support vectors  \n",
    "            # sv_index =  \n",
    "            # decide b using support vectors\n",
    "            \n",
    "            #b =  \n",
    "\n",
    "            #################### END CODE ##############################    \n",
    " \n",
    "          \n",
    "           \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        self.b = b\n",
    "        self.a = a\n",
    "        self.losses = losses\n",
    "    \n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        return (self.a * self.y).dot(self.kernel(self.X, X)) + self.b\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.sign(self.decision_function(X))\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        return np.mean(y == y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train svm using training data\n",
    "y_train[y_train == 0] = -1\n",
    "svm = SVM_Dual(kernel ='rbf',epoches = 500, C = 1)\n",
    "svm.fit(X_train,y_train)\n",
    "\n",
    "# plot evolution of the loss function\n",
    "plt.plot(svm.losses)\n",
    "plt.title(\"loss per epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot classificaiton result of test data\n",
    "plot_classification_region(X_test, svm, ylabels =['y=0','y=1'],alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boudary and margins\n",
    "def plot_decision_boundary(svm): \n",
    "    X_train = svm.X\n",
    "    y_train = svm.y\n",
    "    fig,ax = plt.subplots()\n",
    "    scatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap ='bwr',s = 8, alpha = 0.2, edgecolors = 'none')\n",
    "    ax.legend(scatter.legend_elements()[0],['y=0','y=1'] , loc=\"lower right\", title=\"Classes\")\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "    XX = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    YY = svm.decision_function(XX).reshape(xx1.shape)\n",
    "    ax.contour(xx1,xx2, YY,levels=[-1, 0, 1], linestyles=['--', '-', '--'],colors =['b','g','r'], alpha = 0.8 )\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(svm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce757765b91d5aadb11280eda455f2bacc59ce7bb698ca5d9d25b96a4f6d2ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
